{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd3e724b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Explain quantum computing in one sentence.\n",
      "Claude: Error: Error code: 404 - {'type': 'error', 'error': {'type': 'not_found_error', 'message': 'model: claude-3-5-sonnet-20240620'}, 'request_id': 'req_011CY8jxdnMtZ7ktTrJDR8YQ'}\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "\n",
    "# Initialize the client with your API key\n",
    "client = anthropic.Anthropic(\n",
    "    api_key=\"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\"  # Replace with your actual key\n",
    ")\n",
    "\n",
    "def chat_with_claude(prompt, model=\"claude-3-5-sonnet-20240620\"):\n",
    "    try:\n",
    "        response = client.messages.create(\n",
    "            model=model,\n",
    "            max_tokens=1024,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        # Extract and return the text response\n",
    "        return response.content[0].text\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# --- Test the function ---\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = \"Explain quantum computing in one sentence.\"\n",
    "    print(f\"User: {user_input}\")\n",
    "    \n",
    "    reply = chat_with_claude(user_input)\n",
    "    print(f\"Claude: {reply}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fab48347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Explain reinforcement learning in 2 sentences.\n",
      "Claude: API Error: Error code: 404 - {'type': 'error', 'error': {'type': 'not_found_error', 'message': 'model: claude-3-5-sonnet-20241022'}, 'request_id': 'req_011CY8hUTRFWUAQxWni5AX44'}\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "\n",
    "# Initialize the client with your API key\n",
    "client = anthropic.Anthropic(\n",
    "    api_key=\"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\"  # Replace with your actual key\n",
    ")\n",
    "\n",
    "def chat_with_claude(prompt, model=\"claude-3-5-sonnet-20241022\"):\n",
    "    \"\"\"\n",
    "    Send a prompt to Claude and get a response.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Your question or instruction\n",
    "        model: Claude model ID (default: latest Sonnet)\n",
    "    \n",
    "    Returns:\n",
    "        Claude's text response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.messages.create(\n",
    "            model=model,\n",
    "            max_tokens=1024,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        # Extract the text from the response\n",
    "        return response.content[0].text\n",
    "    \n",
    "    except anthropic.APIError as e:\n",
    "        return f\"API Error: {e}\"\n",
    "\n",
    "# --- Usage Example ---\n",
    "if __name__ == \"__main__\":\n",
    "    user_message = \"Explain reinforcement learning in 2 sentences.\"\n",
    "    \n",
    "    print(f\"User: {user_message}\")\n",
    "    reply = chat_with_claude(user_message)\n",
    "    print(f\"Claude: {reply}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97467675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude: # PPO in Reinforcement Learning\n",
      "\n",
      "**PPO (Proximal Policy Optimization)** is a popular policy gradient algorithm in reinforcement learning developed by OpenAI in 2017.\n",
      "\n",
      "## Key Concepts\n",
      "\n",
      "**Main Goal:** Update the policy to improve performance while preventing destructively large policy updates that could hurt training stability.\n",
      "\n",
      "## How It Works\n",
      "\n",
      "PPO constrains policy updates by:\n",
      "\n",
      "1. **Clipped Objective Function:**\n",
      "   ```\n",
      "   L^CLIP(θ) = E[min(r(θ)·A, clip(r(θ), 1-ε, 1+ε)·A)]\n",
      "   ```\n",
      "   - `r(θ)` = probability ratio between new and old policy\n",
      "   - `A` = advantage function (how good an action was)\n",
      "   - `ε` = clipping parameter (typically 0.2)\n",
      "\n",
      "2. **Trust Region:** The clipping ensures the new policy doesn't deviate too far from the old one\n",
      "\n",
      "## Advantages\n",
      "\n",
      "- ✅ **Simpler** than TRPO (Trust Region Policy Optimization)\n",
      "- ✅ **More stable** than vanilla policy gradient methods\n",
      "- ✅ **Sample efficient**\n",
      "- ✅ **Easy to implement and tune**\n",
      "- ✅ **Works well across diverse tasks**\n",
      "\n",
      "## Common Applications\n",
      "\n",
      "- Robotics control\n",
      "- Game playing (Dota 2, hide-and-seek)\n",
      "- ChatGPT/LLM fine-tuning (RLHF)\n",
      "- Continuous and discrete action spaces\n",
      "\n",
      "PPO has become one of the default choices for RL practitioners due to its balance of simplicity, stability, and performance.\n",
      "Claude: # Main Limitations of PPO\n",
      "\n",
      "## 1. **Sample Inefficiency**\n",
      "- Requires **many environment interactions** to learn\n",
      "- Much less sample-efficient than off-policy methods (SAC, TD3, DQN)\n",
      "- Can be impractical for expensive simulations or real-world training\n",
      "- May need millions of steps even for relatively simple tasks\n",
      "\n",
      "## 2. **Hyperparameter Sensitivity**\n",
      "- Performance heavily depends on tuning:\n",
      "  - Learning rate\n",
      "  - Clipping parameter (ε)\n",
      "  - Number of epochs per update\n",
      "  - Batch size and minibatch size\n",
      "  - GAE parameters (λ, γ)\n",
      "- Optimal settings vary significantly across different tasks\n",
      "- Can require substantial experimentation\n",
      "\n",
      "## 3. **Local Optima**\n",
      "- As an on-policy method, prone to getting stuck in **local optima**\n",
      "- Limited exploration once policy becomes confident\n",
      "- May settle for suboptimal solutions early in training\n",
      "\n",
      "## 4. **Computational Cost**\n",
      "- Needs **multiple epochs** over collected data for each update\n",
      "- Higher wall-clock time compared to simpler policy gradient methods\n",
      "- Can be slow for large neural networks\n",
      "\n",
      "## 5. **Scaling Challenges**\n",
      "- **Credit assignment** problems in long-horizon tasks\n",
      "- Difficulty with very high-dimensional action spaces\n",
      "- Struggles with tasks requiring precise, fine-grained control\n",
      "\n",
      "## 6. **Reward Engineering**\n",
      "- Still sensitive to **reward shaping**\n",
      "- Sparse rewards remain challenging\n",
      "- May require careful reward design for complex tasks\n",
      "\n",
      "## When to Consider Alternatives\n",
      "- **Sample efficiency critical?** → Use SAC, TD3, or DQN (off-policy)\n",
      "- **Need exploration?** → Add curiosity-driven methods\n",
      "- **Very complex tasks?** → Consider hierarchical RL or model-based methods\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic(api_key=\"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\" ) # Replace with your actual key\")\n",
    "\n",
    "def conversation(messages, model=\"claude-sonnet-4-5-20250929\"):\n",
    "    response = client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=2048,\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.content[0].text\n",
    "\n",
    "# Conversation history\n",
    "history = []\n",
    "\n",
    "# First message\n",
    "history.append({\"role\": \"user\", \"content\": \"What is PPO in RL?\"})\n",
    "reply = conversation(history)\n",
    "print(f\"Claude: {reply}\")\n",
    "\n",
    "# Add Claude's response to history\n",
    "history.append({\"role\": \"assistant\", \"content\": reply})\n",
    "\n",
    "# Second message (continues context)\n",
    "history.append({\"role\": \"user\", \"content\": \"What are its main limitations?\"})\n",
    "reply = conversation(history)\n",
    "print(f\"Claude: {reply}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ab715d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ claude-3-5-sonnet-20240620 - NOT FOUND\n",
      "❌ claude-3-opus-20240229 - NOT FOUND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q0/6fdy9rxn5212mzd1x1nd96t40000gn/T/ipykernel_64280/1676723828.py:16: DeprecationWarning: The model 'claude-3-opus-20240229' is deprecated and will reach end-of-life on January 5th, 2026.\n",
      "Please migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\n",
      "  response = client.messages.create(\n",
      "/var/folders/q0/6fdy9rxn5212mzd1x1nd96t40000gn/T/ipykernel_64280/1676723828.py:16: DeprecationWarning: The model 'claude-3-sonnet-20240229' is deprecated and will reach end-of-life on July 21st, 2025.\n",
      "Please migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\n",
      "  response = client.messages.create(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ claude-3-sonnet-20240229 - NOT FOUND\n",
      "✅ claude-3-haiku-20240307 - WORKS\n",
      "✅ claude-sonnet-4-5-20250929 - WORKS\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic(api_key=\"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\")\n",
    "\n",
    "# Try a few common model IDs\n",
    "test_models = [\n",
    "    \"claude-3-5-sonnet-20240620\",\n",
    "    \"claude-3-opus-20240229\",\n",
    "    \"claude-3-sonnet-20240229\",\n",
    "    \"claude-3-haiku-20240307\",\n",
    "    \"claude-sonnet-4-5-20250929\"\n",
    "]\n",
    "\n",
    "for model_id in test_models:\n",
    "    try:\n",
    "        response = client.messages.create(\n",
    "            model=model_id,\n",
    "            max_tokens=10,\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Hi\"}]\n",
    "        )\n",
    "        print(f\"✅ {model_id} - WORKS\")\n",
    "    except anthropic.NotFoundError:\n",
    "        print(f\"❌ {model_id} - NOT FOUND\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ {model_id} - Error: {type(e).__name__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa235f19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
